{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping AACN Sub-Competencies Using Semantic Similarity Scores\n",
        "\n",
        "## Introduction\n",
        "\n",
        "* I'm Chris Macintosh\n",
        "* Director of Nursing Informatics MS at University of Utah College of Nursing\n",
        "* I'm going to demonstrate how to generate semantic similarity scores (cosine similarity) using an open-source large language model (LLM)\n",
        "\n",
        "## Semantic Similarity\n",
        "\n",
        "* The lines between quantitative and qualitiative analysis are becomming blurred.\n",
        "* LLMs can be used to perform quantitative analysis on similarity in meaning of textual data.\n",
        "* This is a demo of calculating semantic similarity scores for different groups of sentences.\n",
        "\n",
        "## AACN Essentials\n",
        "\n",
        "* AACN outlined 10 domains for undergraduate and graduate level nursing competencies.\n",
        "* Those 10 domains contain 45 competencies.\n",
        "* The 45 competencies are further defined into sub-competencies.\n",
        "* There are 204 graduate-level sub-competencies.\n",
        "* Mapping that many sub-competencies is not a trivial task.\n",
        "* This proof of concept report maps AACN subcompetencies to course learning outcomes by ranking them by cosine similarity score.\n",
        "\n",
        "## Sentence Embedding\n",
        "\n",
        "* Sentence embedding involves replacing words with arrays of numbers called vectors.\n",
        "* Mathematical techniques can be done to compare vectors with each other.\n",
        "* Older methods used sparce vectors that consisted mostly of zeros.\n",
        "* Older methods called \"bag of words\" focused on words present, but ignored word order.\n",
        "* More sophisticated methods like if-idf (term frequency - inverse document frequency) and BM25 (best match 25) expanded on \"bag of words\" and incorporated significance of words.\n",
        "* More sophisticated neural network methods result in dense vectors (non-zero vectors) that identify location of a sentence in a large vector space.\n",
        "* Dense vector methods can encode semantics.\n",
        "* BERT (bidirectional encoder representations from transformers) also encode how words relate to each other.\n",
        "* SBERT (sentence - BERT) is an improvement on BERT that speeds up comparisons.\n",
        "\n",
        "## Overview of the Steps\n",
        "\n",
        "* Sentence vector embeddings are created.\n",
        "* Cosine similarity scores are calculated.\n",
        "* Scores are exported to and Excel workbook so formatting could be used to highlight important aspects and lists could be sorted.\n",
        "\n",
        "## Google Colaboratory\n",
        "\n",
        "* Code is run on a Jupyter Notebook in Google Colab.\n",
        "\n",
        "## File Preparation\n",
        "\n",
        "* I created a data file using Excel.\n",
        "* A separate sheet for each group of sentences was created.\n",
        "* Each sentence was assigned a unique identifier.\n",
        "* Both sheets have an id column and a description column.\n",
        "* This could be done using any two groups of sentences.\n",
        "\n",
        "## Options for Loading Files\n",
        "\n",
        "Select one of the options below.\n",
        "\n",
        "### Option #1: Clone GitHub Repository\n",
        "\n"
      ],
      "metadata": {
        "id": "vnBCbZybEuxo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K0RYPHlElNX"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cmcntsh/SemanticSimilarityReport_ALNI_NIWG_2024-09.git # replace the address shown with the address to your own repository"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option #2: Use Command in Jupyter Notebook"
      ],
      "metadata": {
        "id": "rmspKZyOsYjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "3FeN5EJDso3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option #3: Use Google Colab Interface\n",
        "\n",
        "(Not demonstrated here.)\n",
        "\n",
        "### Option #4: Upload from Google Drive\n",
        "\n",
        "(Not demonstrated here.)"
      ],
      "metadata": {
        "id": "aip0zWEstgmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File Path"
      ],
      "metadata": {
        "id": "_HmNs5EqQ_Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/SemanticSimilarityReport_ALNI_NIWG_2024-09/InformaticsCompetenciesSheet.xlsx'"
      ],
      "metadata": {
        "id": "XbTo9VmzRB74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Sentence Transformers"
      ],
      "metadata": {
        "id": "K1NU-XSaRnnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only needs to run if not already installed.\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "suG9BILpRsDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read File"
      ],
      "metadata": {
        "id": "vN7P5b5uSD38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data from Excel workbook.\n",
        "# There should be two sheets (Sheet1, Sheet2) with headings id & description.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "DataFile = file_path\n",
        "\n",
        "df1 = pd.read_excel(DataFile, 'Sheet1')\n",
        "df2 = pd.read_excel(DataFile, 'Sheet2')\n",
        "\n",
        "# Add the ids and text chunks to lists.\n",
        "Sheet1_id = df1.id.values.tolist()\n",
        "Sheet1_text = df1.description.values.tolist()\n",
        "Sheet2_id = df2.id.values.tolist()\n",
        "Sheet2_text = df2.description.values.tolist()"
      ],
      "metadata": {
        "id": "GXK9o6_QSGPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the Model"
      ],
      "metadata": {
        "id": "TtapLAZoSaNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Models - https://huggingface.co/models?library=sentence-transformers\n",
        "model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "bG9KDRdtSc2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Sentence Embeddings"
      ],
      "metadata": {
        "id": "x-noJlXquUCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings1 = model.encode(Sheet1_text)\n",
        "embeddings2 = model.encode(Sheet2_text)"
      ],
      "metadata": {
        "id": "5CUIUiNUuaJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate Cosine Similarity Scores"
      ],
      "metadata": {
        "id": "_-nXcIJSuqRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.util import cos_sim\n",
        "\n",
        "scores = cos_sim(embeddings1, embeddings2)"
      ],
      "metadata": {
        "id": "E4OLkK04uuHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataframes"
      ],
      "metadata": {
        "id": "uSxD_zV3u1kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "scores_df = pd.DataFrame(scores.numpy(), index = Sheet1_id, columns = Sheet2_id)\n",
        "scores_df\n",
        "transposed_df = scores_df.transpose()\n",
        "#transposed_df"
      ],
      "metadata": {
        "id": "fUuX_DNyu7Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(scores_df))\n",
        "print(len(transposed_df))\n",
        "print(scores_df.shape)"
      ],
      "metadata": {
        "id": "V91RyWaDvBYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate the Excel Report"
      ],
      "metadata": {
        "id": "-gYKv209vJu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter"
      ],
      "metadata": {
        "id": "u8pOJUuwvM7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.path.split(DataFile)[0])"
      ],
      "metadata": {
        "id": "aGVlJ4PNvuYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path and name for the Excel workbook to create.\n",
        "OutputFile = os.path.split(DataFile)[0] + \"\\CosineSimilarity_\" + os.path.split(DataFile)[1]\n",
        "\n",
        "\n",
        "# Determine column letters from column numbers\n",
        "# https://stackoverflow.com/questions/29351492/how-to-make-a-continuous-alphabetic-list-python-from-a-z-then-from-aa-ab-ac-e\n",
        "def char_label(n, chars):\n",
        "    indexes = []\n",
        "    while n:\n",
        "        residual = n % len(chars)\n",
        "        if residual == 0:\n",
        "            residual = len(chars)\n",
        "        indexes.append(residual)\n",
        "        n = (n - residual)\n",
        "        n = n // len(chars)\n",
        "    indexes.reverse()\n",
        "    label = ''\n",
        "    for i in indexes:\n",
        "        label += chars[i-1]\n",
        "    return label\n",
        "\n",
        "my_chrs = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "#start_col = 2\n",
        "#end_col = start_col + 204\n",
        "#start_row = 10\n",
        "#end_row = start_row + 27\n",
        "#for col in range(start_col, end_col+1):\n",
        "#    r=char_label(col, my_chrs)\n",
        "#    print(r + str(1) + \" = max(\" + r + str(start_row) + \":\" + r + str(end_row) + \")\")\n",
        "\n",
        "# Use a with statement to close the file automatically when all file writing and formatting is complete.\n",
        "with pd.ExcelWriter(OutputFile) as writer:\n",
        "\n",
        "    # Write the scores dataframe to the Excel workbook. Leave blank rows at the top.\n",
        "    scores_df.to_excel(writer, sheet_name=\"Sheet1\", startrow=10, startcol=0)\n",
        "\n",
        "    # Get the number of rows and columns\n",
        "    (max_row, max_col) = scores_df.shape\n",
        "\n",
        "    # Load the sheet as an object to do some formatting later\n",
        "    sheet1 = writer.sheets['Sheet1']\n",
        "\n",
        "    # Label the first row as \"Max\". I'll identify the highest match in Excel later.\n",
        "    sheet1.write(0,0,\"Max\")\n",
        "\n",
        "    # Add formula for Max value in column in first row.\n",
        "    for col in range(2, max_col+2):\n",
        "        r=char_label(col, my_chrs)\n",
        "        sheet1.write(0, col-1, \"= max(\" + r + str(12) + \":\" + r + str(11+max_row) + \")\")\n",
        "\n",
        "    sheet1.conditional_format(0,1, max_row+10, max_col, {\"type\": \"3_color_scale\", \"mid_color\": \"#FFFFFF\", \"max_color\": \"#008000\", \"min_color\": \"#FFFF00\", \"min_value\": -1, \"max_value\": 1, \"mid_value\": 0})\n",
        "\n",
        "    # Set up conditional formatting in the worksheet.\n",
        "    sheet1.conditional_format(10,1, max_row+10, max_col, {\"type\": \"3_color_scale\", \"mid_color\": \"#FFFFFF\", \"max_color\": \"#0000FF\", \"min_color\": \"#FFFF00\", \"min_value\": -1, \"max_value\": 1, \"mid_value\": 0})\n",
        "\n",
        "    # Add the description text as comments for the rows.\n",
        "    for i in range(len(Sheet1_text)):\n",
        "        sheet1.write_comment(i+11,0,Sheet1_text[i], {'text_wrap': 0, \"x_scale\": 2, \"y_scale\": 2})\n",
        "\n",
        "    # Add the description text as comments for the columns.\n",
        "    for i in range(len(Sheet2_text)):\n",
        "        sheet1.write_comment(10,i+1,Sheet2_text[i], {'text_wrap': 0, \"x_scale\": 2, \"y_scale\": 2})\n",
        "\n",
        "    # Add the transposed data to a second sheet with same formatting as first sheet.\n",
        "    transposed_df.to_excel(writer, sheet_name=\"Sheet2\", startrow=10, startcol=0)\n",
        "    (max_row, max_col) = transposed_df.shape\n",
        "    sheet2 = writer.sheets['Sheet2']\n",
        "    sheet2.write(0,0,\"Max\")\n",
        "\n",
        "    # Add formula for Max value in column in first row.\n",
        "    for col in range(2, max_col+2):\n",
        "        r=char_label(col, my_chrs)\n",
        "        sheet2.write(0, col-1, \"= max(\" + r + str(12) + \":\" + r + str(11+max_row) + \")\")\n",
        "\n",
        "    sheet2.conditional_format(0,1, max_row+10, max_col, {\"type\": \"3_color_scale\", \"mid_color\": \"#FFFFFF\", \"max_color\": \"#008000\", \"min_color\": \"#FFFF00\", \"min_value\": -1, \"max_value\": 1, \"mid_value\": 0})\n",
        "\n",
        "    sheet2.conditional_format(10,1, max_row+10, max_col, {\"type\": \"3_color_scale\", \"mid_color\": \"#FFFFFF\", \"max_color\": \"#0000FF\", \"min_color\": \"#FFFF00\", \"min_value\": -1, \"max_value\": 1, \"mid_value\": 0})\n",
        "    for i in range(len(Sheet2_text)):\n",
        "        sheet2.write_comment(i+11,0,Sheet2_text[i], {'text_wrap': 0, \"x_scale\": 2, \"y_scale\": 2})\n",
        "    for i in range(len(Sheet1_text)):\n",
        "        sheet2.write_comment(10,i+1,Sheet1_text[i], {'text_wrap': 0, \"x_scale\": 2, \"y_scale\": 2})\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "s0A9xk6JvQ1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Report"
      ],
      "metadata": {
        "id": "hJc-218_v_4H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Pccf--ywH0C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}